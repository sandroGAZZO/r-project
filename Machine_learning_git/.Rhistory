test <- wines_train[i,]
pred <- knn_pond(kseq[k],"epan","max",train[,-1],test[,-1],train$Classe)
err[k] <- err[k] + (pred$fitted.values!=test$Classe)
}
err[k] <- err[k]/nrow(wines_train)
}
derr <- data.frame(k=kseq,error=err)
ggplot(data=derr,aes(x=k,y=error)) + geom_line()# + xlim(0,50)
kopt <- kseq[which.min(err)]
# Application sur la base de test
knn_test <- kknn(as.factor(Classe) ~ ., wines_train, wines_test,distance=2, kernel="rectangular")
mc <- table(wines_test$Class,knn_test)
mc
# Exo 2
# Base de donnÃ©es SPAM
library(kernlab)
data(spam)
library(rpart)
library(rpart.plot)
# On dÃ©finit les tailles des Ã©chantillons d'apprentissage et de test
ntot <- nrow(spam)
ntrain <- floor(0.75*ntot)
ntest <- ntot-ntrain
# On tire au sort parmi toutes les lignes de la base, celles qui feront partie de l'Ã©chantillon d'apprentissage
ind_train <- sample(1:ntot,ntrain)
spam_train <- spam[ind_train,]
spam_test <- spam[-ind_train,]
# On utilise la fonction rpart pour construire l'arbre CART
# On fixe cp=0 pour obtenir toute la sÃ©quence de sous-arbres emboÃ®tÃ©s
spam_cart <- rpart(type~.,data=spam_train, cp = 0)
rpart.plot(spam_cart)
printcp(spam_cart)
plotcp(spam_cart)
# On retient la valeur de cp Ã  partir de laquelle l'erreur se stabilise (cp=0.0037 sur cet exemple, mais cela dÃ©pend du tirage de l'Ã©chantillon ...)
spam_cart_opt <- prune(spam_cart,cp=0.0037)
rpart.plot(spam_cart_opt,extra=3,cex=1)
# Evaluation sur donnÃ©es test
pred_test <- predict(spam_cart_opt,newdata=spam_test,type="class")
# matrice de confusion
mc <- table(pred_test,spam_test$type,dnn=c("PrÃ©dit","Vrai"))
# en pourcentages
prop.table(mc)
# taux de mauvais classement
taux_erreur <- 1 - sum(diag(mc))/sum(mc)  # 9.2%
# Exo 2
# Base de donnÃ©es SPAM
library(kernlab)
install.packages(kernlab)
install.packages("kernlab")
# Exo 2
# Base de donnÃ©es SPAM
library(kernlab)
data(spam)
library(rpart)
library(rpart.plot)
# On dÃ©finit les tailles des Ã©chantillons d'apprentissage et de test
ntot <- nrow(spam)
ntrain <- floor(0.75*ntot)
ntest <- ntot-ntrain
# On tire au sort parmi toutes les lignes de la base, celles qui feront partie de l'Ã©chantillon d'apprentissage
ind_train <- sample(1:ntot,ntrain)
spam_train <- spam[ind_train,]
spam_test <- spam[-ind_train,]
# On utilise la fonction rpart pour construire l'arbre CART
# On fixe cp=0 pour obtenir toute la sÃ©quence de sous-arbres emboÃ®tÃ©s
spam_cart <- rpart(type~.,data=spam_train, cp = 0)
rpart.plot(spam_cart)
printcp(spam_cart)
plotcp(spam_cart)
# On retient la valeur de cp Ã  partir de laquelle l'erreur se stabilise (cp=0.0037 sur cet exemple, mais cela dÃ©pend du tirage de l'Ã©chantillon ...)
spam_cart_opt <- prune(spam_cart,cp=0.0037)
rpart.plot(spam_cart_opt,extra=3,cex=1)
# Evaluation sur donnÃ©es test
pred_test <- predict(spam_cart_opt,newdata=spam_test,type="class")
# matrice de confusion
mc <- table(pred_test,spam_test$type,dnn=c("PrÃ©dit","Vrai"))
# en pourcentages
prop.table(mc)
# taux de mauvais classement
taux_erreur <- 1 - sum(diag(mc))/sum(mc)  # 9.2%
# Exo 2
# Base de donnÃ©es SPAM
rpart(1)
# Exo 2
# Base de donnÃ©es SPAM
rpart()
# Exo 2
# Base de donnÃ©es SPAM
npart(1)
# Exo 2
# Base de donnÃ©es SPAM
part(1)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
?randomForest
#setwd
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
ani<-read.csv("C:/Users/Sandro/Documents/R/Meth_app/projet/animaux_I.csv")
summary(ani)
str(ani)
ani<-ani[,-1]
stock_ani<-ani
ani$hair<-as.factor(ani$hair)
ani$feathers<-as.factor(ani$feathers)
ani$eggs<-as.factor(ani$eggs)
ani$milk<-as.factor(ani$milk)
ani$airborne<-as.factor(ani$airborne)
ani$aquatic<-as.factor(ani$aquatic)
ani$predator<-as.factor(ani$predator)
ani$toothed<-as.factor(ani$toothed)
ani$backbone<-as.factor(ani$backbone)
ani$breathes<-as.factor(ani$breathes)
ani$venomous<-as.factor(ani$venomous)
ani$fins<-as.factor(ani$fins)
ani$tail<-as.factor(ani$tail)
ani$domestic<-as.factor(ani$domestic)
ani$catsize<-as.factor(ani$catsize)
ani$class_type<-as.factor(ani$class_type)
str(ani)
summary(ani)
stock_sans_nom_ani<-ani
levels(ani$class_type)<-c("Mammifère","Oiseau","Reptile",
"Poisson","Amphibien","Insecte","Invertébré")
summary(ani)
str(ani)
indtrain<-createDataPartition(ani$class_type,p=0.75,list=F)
ani.appr <- ani[indtrain,]
ani.test <- ani[-indtrain,]
summary(ani.appr)
summary(ani.test)
prop.table(table(ani.appr$class_type))
prop.table(table(ani.test$class_type))
summary(ani)
##################################
#####               ##############
##### Gazzo Sandro  ##############
#####      &        ##############
##### Dellouve Théo ##############
#####               ##############
##################################
#### LIBRAIRIES ##################
# Librairies utilisées dans ce programme
# Si une des librairies ne peut être lancée, utilisez
# la fonction install.packages
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
## Chargement des données ##
ani<-read.csv("C:/Users/Sandro/Documents/R/Meth_app/projet/animaux_I.csv")
## Etude brute des données ##
summary(ani)
str(ani)
## On supprime la colonne des noms des espèces
ani<-ani[,-1]
# on passe en facteur les données codées en 0 et 1
ani$hair<-as.factor(ani$hair)
ani$feathers<-as.factor(ani$feathers)
ani$eggs<-as.factor(ani$eggs)
ani$milk<-as.factor(ani$milk)
ani$airborne<-as.factor(ani$airborne)
ani$aquatic<-as.factor(ani$aquatic)
ani$predator<-as.factor(ani$predator)
ani$toothed<-as.factor(ani$toothed)
ani$backbone<-as.factor(ani$backbone)
ani$breathes<-as.factor(ani$breathes)
ani$venomous<-as.factor(ani$venomous)
ani$fins<-as.factor(ani$fins)
ani$tail<-as.factor(ani$tail)
ani$domestic<-as.factor(ani$domestic)
ani$catsize<-as.factor(ani$catsize)
ani$class_type<-as.factor(ani$class_type)
str(ani)
summary(ani)
# on affecte les noms des catégories d'animaux pour rendre le contenu
# visuellement plus agréable
levels(ani$class_type)<-c("Mammifère","Oiseau","Reptile",
"Poisson","Amphibien","Insecte","Invertébré")
# on vérifie nos changements
summary(ani)
str(ani)
## Création de la base d'apprentissage et de la base de test
# on fait en sorte qu'il y a bien 75% par type d'espèces dans la
# base d'apprentissage et 25% dans la base de test pour éviter
#☻ certains désagréments (aucun ampphibien dans la base de test par exemple)
indtrain<-createDataPartition(ani$class_type,p=0.75,list=F)
ani.appr <- ani[indtrain,]
ani.test <- ani[-indtrain,]
# on regarde un peu plus attentivement les deux bases créées
summary(ani.appr)
summary(ani.test)
# on regarde si la proportion est à peu près équivalente
prop.table(table(ani$class__type))
prop.table(table(ani.appr$class_type))
prop.table(table(ani.test$class_type))
# on regarde si la proportion est à peu près équivalente
prop.table(ani$class__type)
?prop.table
ani
prop.table(ani$class_type)
prop.table(data.frame(ani$class_type))
ani$class_type
prop.table(ani$class_type)
library(mclust)
library(Rmixmod)
#Exo1
p<-1/4
mu1<- c(0,1)
mu2<- c(1,1)
mu3<- c(0,-1)
mu4<- c(0,-2)
sigma<-matrix(c(0.2,0,0,0.1),ncol=2)
n=100
x<-matrix(0,nr=n,nc=2)
comp<-sample(1:4,n,replace=T)
for (i in 1:n)
{
if (comp[i]==1) # simulation pour les donnÃ©es de la composante 1
{
x[i,] <- c(rnorm(1,0,sqrt(0.2)),rnorm(1,1,sqrt(0.1)))
}else if (comp[i]==2) # simulation pour les donnÃ©es de la composante 2
{
x[i,] <- c(rnorm(1,1,sqrt(0.2)),rnorm(1,1,sqrt(0.1)))
}
else if (comp[i]==3) # simulation pour les donnÃ©es de la composante 2
{
x[i,] <- c(rnorm(1,0,sqrt(0.2)),rnorm(1,-1,sqrt(0.1)))
}
else{ # simulation pour les donnÃ©es de la composante 3
x[i,] <- c(rnorm(1,0,sqrt(0.2)),rnorm(1,-2,sqrt(0.1)))
}
}
mu_vrai<-matrix(c(0,1,1,1,0,-1,0,-2),nr=4,byrow=T)
sigma_vrai<-matrix(rep(c(0.2,0,0,0.1),4),nr=4,byrow=T)
# Nuage de points, par la fonction plot de R ou par le package ggplot
plot(x,type="p")
require(ggplot2)
x<-data.frame(x)
ggplot(data=x,aes(x[,1],x[,2],color=as.factor(comp))) + geom_point()
#### mclust ########
# avec reglage par defaut
mc<-Mclust(x)
table(comp)
summary(mc)
plot(mc,what="BIC")
summary(mc$BIC)
plot(mc,what="classification")
summary(mc$classification)
plot(mc,what="uncertainty")
summary(mc$uncertainty)
mcICL<-mclustICL(x)
summary(mcICL)
plot(mcICL)
# pour faire classification
str(mcICL)
mc15<-Mclust(x,G=1:5)
summary(mc15)
plot(mc15,what="BIC")
summary(mc15$BIC)
mcICL15<-mclustICL(x,G=1:5)
summary(mcICL15)
plot(mcICL15)
mc1<-Mclust(x,G=1,modelNames = "EEI")
plot(mc1,what="BIC")
summary(mc1$BIC)
plot(mc1,what="classification")
summary(mc1)
mc2<-Mclust(x,G=2,modelNames = "EEI")
plot(mc2,what="BIC")
summary(mc2$BIC)
plot(mc2,what="classification")
summary(mc2)
mc3<-Mclust(x,G=3,modelNames = "EEI")
plot(mc3,what="BIC")
summary(mc3$BIC)
plot(mc3,what="classification")
summary(mc3)
mc4<-Mclust(x,G=4,modelNames = "EEI")
plot(mc4,what="BIC")
summary(mc4$BIC)
plot(mc4,what="classification")
summary(mc4)
mc5<-Mclust(x,G=5,modelNames = "EEI")
plot(mc5,what="BIC")
summary(mc4$BIC)
plot(mc5,what="classification")
summary(mc5)
## sur plusieurs directement, mais on ne voit pas
# comment focntionne Ã©tape par Ã©tape l'algorithme
mctest<-Mclust(x,G=1:5,modelNames = "EEI")
plot(mctest,what="BIC")
summary(mctest$BIC)
plot(mctest,what="classification")
##### Rmixmod #######
rmm<-mixmodCluster(x,nbCluster=1:5)
summary(rmm)
plot(rmm)
plotCluster(rmm["bestResult"],data=x)
rmm4<-mixmodCluster(x,nbCluster=4)
summary(rmm4)
plot(rmm4)
plotCluster(rmm4["bestResult"],data=x)
rmmICL<-mixmodCluster(x,nbCluster=1:5,criterion="ICL")
summary(rmmICL)
plot(rmmICL)
plotCluster(rmmICL["bestResult"],data=x)
rmmICL4<-mixmodCluster(x,nbCluster=4,criterion="ICL")
summary(rmmICL4)
plot(rmmICL4)
plotCluster(rmmICL4["bestResult"],data=x)
rmmEM<-mixmodCluster(x,nbCluster=1:5,strategy=mixmodStrategy("EM"))
rmmEM<-mixmodCluster(x,nbCluster=1:5,strategy=mixmodStrategy("EM"))
rmmCEM<-mixmodCluster(x,nbCluster=1:5,strategy=mixmodStrategy("CEM"))
summary(rmmEM)
summary(rmmCEM)
plot(rmmEM)
plot(rmmCEM)
plotCluster(rmmEM["bestResult"],data=x)
plotCluster(rmmCEM["bestResult"],data=x)
## Exo 2
data(birds)
head(birds)
summary(birds)
rmm_birds<-mixmodCluster(birds,nbCluster=1:5)
summary(rmm_birds)
plot(rmm_birds)
#plotCluster(rmm_birds["bestResult"],data=birds) ==> pas possible ici
barplot(rmm_birds)
library(jpeg)
install.packages(jpeg)
install.packages("jpeg")
install.packages("imager")
install.packages("dbscan")
dim(plante)
library(jpeg)
library(imager)
library(dbscan)
source("C:/Users/Sandro/Documents/R/Meth_app/color_utils.R")
setwd("C:/Users/Sandro/Documents/R/Meth_app")
plante<-readJPEG("arabidopsis.jpeg",native = FALSE)
plante
dim(plante)
display(plante)
plante[115:120,115:120,1]
plante[115:120,115:120,2]
plante[115:120,115:120,3]
# mat<-c()
# mat<-matrix(ncol=(221*228),nrow=3)
#
# for (i in 1:3){
#   for (j in 1:221){
#     for (k in 1:228){
#       mat[i,k+(j-1)*228]=plante[j,k,i]
#
#     }
#   }
# }
#
# dim(mat)
# mat
# mat[1,120+118*128]
var1<-as.vector(plante[,,1])
var2<-as.vector(plante[,,2])
var3<-as.vector(plante[,,3])
plante_data<-cbind(var1,var2,var3)
dim(plante_data)
dev.off()
kNNdistplot(plante_data,10)
dknn <- kNNdist(plante_data,k=10)
plot(sort(dknn,decreasing = T),type='l',ylim=c(0,0.05))
kNNdistplot(plante_data,100)
dev.off()
dknn <- kNNdist(plante_data,k=100)
plot(sort(dknn,decreasing = T),type='l',ylim=c(0,0.05))
dev.off()
dknn <- kNNdist(plante_data,k=200)
plot(sort(dknn,decreasing = T),type='l',ylim=c(0,0.05))
eps<-0.015
dbplante<- dbscan(plante_data,eps=eps,minPts = 100)
unique(dbplante$cluster)
length(dbplante$cluster)
matcl<-matrix(dbplante$cluster,nrow=dim(plante)[1],ncol=dim(plante)[2],byrow=F)
image(matcl)
eps<-0.03
dbplante<- dbscan(plante_data,eps=eps,minPts = 100)
unique(dbplante$cluster)
length(dbplante$cluster)
matcl<-matrix(dbplante$cluster,nrow=dim(plante)[1],ncol=dim(plante)[2],byrow=F)
image(matcl)
eps<-0.02
dbplante<- dbscan(plante_data,eps=eps,minPts = 20)
unique(dbplante$cluster)
length(dbplante$cluster)
matcl<-matrix(dbplante$cluster,nrow=dim(plante)[1],ncol=dim(plante)[2],byrow=F)
image(matcl)
dbplante$cluster[1] # classe du premier pixel en haut ? gauche ->1
#je d?finis la plante comme tout ce qui n'est pas dans la classe 1
prop.table(table(dbplante$cluster))
#surface foliaire
(1-prop.table(table(dbplante$cluster))[2])*105
### Exo 2
source("datasets2.R")
d1<- circles(200)
d2<-difftaille(200)
d3<-carres(200)
d4<-losanges(200)
library(ggplot2)
source("C:/Users/Sandro/Documents/R/Meth_app/color_utils.R")
setwd("C:/Users/Sandro/Documents/R/Meth_app")
plante<-readJPEG("arabidopsis.jpeg",native = FALSE)
plante
dim(plante)
display(plante)
plante[115:120,115:120,1]
plante[115:120,115:120,2]
plante[115:120,115:120,3]
var1<-as.vector(plante[,,1])
var2<-as.vector(plante[,,2])
var3<-as.vector(plante[,,3])
plante_data<-cbind(var1,var2,var3)
dim(plante_data)
dev.off()
kNNdistplot(plante_data,10)
dknn <- kNNdist(plante_data,k=10)
plot(sort(dknn,decreasing = T),type='l',ylim=c(0,0.05))
kNNdistplot(plante_data,100)
dev.off()
dknn <- kNNdist(plante_data,k=100)
plot(sort(dknn,decreasing = T),type='l',ylim=c(0,0.05))
dev.off()
dknn <- kNNdist(plante_data,k=200)
plot(sort(dknn,decreasing = T),type='l',ylim=c(0,0.05))
eps<-0.015
dbplante<- dbscan(plante_data,eps=eps,minPts = 100)
unique(dbplante$cluster)
length(dbplante$cluster)
matcl<-matrix(dbplante$cluster,nrow=dim(plante)[1],ncol=dim(plante)[2],byrow=F)
image(matcl)
eps<-0.03
dbplante<- dbscan(plante_data,eps=eps,minPts = 100)
unique(dbplante$cluster)
length(dbplante$cluster)
matcl<-matrix(dbplante$cluster,nrow=dim(plante)[1],ncol=dim(plante)[2],byrow=F)
image(matcl)
eps<-0.02
dbplante<- dbscan(plante_data,eps=eps,minPts = 20)
unique(dbplante$cluster)
length(dbplante$cluster)
matcl<-matrix(dbplante$cluster,nrow=dim(plante)[1],ncol=dim(plante)[2],byrow=F)
image(matcl)
dbplante$cluster[1] # classe du premier pixel en haut ? gauche ->1
#je d?finis la plante comme tout ce qui n'est pas dans la classe 1
prop.table(table(dbplante$cluster))
#surface foliaire
(1-prop.table(table(dbplante$cluster))[2])*105
source("datasets2.R")
d1<- circles(200)
d2<-difftaille(200)
d3<-carres(200)
d4<-losanges(200)
library(ggplot2)
install.packages("ggplot2")
source("datasets2.R")
d1<- circles(200)
d2<-difftaille(200)
d3<-carres(200)
d4<-losanges(200)
library(ggplot2)
d<-rbind(d1,d2,d3,d4)
d$data<- c(rep(c("Cercles","Taille diff?rentes","Carr?s","Losanges"),each=400))
ggplot(data=d,aes(x1,x2))+geom_point() +facet_wrap(~data,scales="free")+ theme(legend.position = "none")
clust_d2<-kmeans(d2,centers=2,nstart = 1)
res<- cbind(d2,clust=clust_d2$cluster)
ggplot(data = res, aes(x1,x2,color=as.factor(clust)))+ geom_point()
sse <- numeric()
for (i in 1:10){
sse[i]=sum(kmeans(d3,centers=i,nstart = 20)$withinss)
}
plot(1:10,sse,pch=20,type="b")
clust_d3<-kmeans(d3,centers=3,nstart=1)
res<- cbind(d3,clust=clust_d3$cluster)
ggplot(data = res, aes(x1,x2,color=as.factor(clust)))+ geom_point()
d<-rbind(d1,d2,d3,d4)
d$data<- c(rep(c("Cercles","Taille diff?rentes","Carr?s","Losanges"),each=400))
ggplot(data=d,aes(x1,x2))+geom_point() +facet_wrap(~data,scales="free")+ theme(legend.position = "none")
clust_d2<-kmeans(d2,centers=2,nstart = 1)
res<- cbind(d2,clust=clust_d2$cluster)
ggplot(data = res, aes(x1,x2,color=as.factor(clust)))+ geom_point()
sse <- numeric()
for (i in 1:10){
sse[i]=sum(kmeans(d3,centers=i,nstart = 20)$withinss)
}
plot(1:10,sse,pch=20,type="b")
sse <- numeric()
for (i in 1:10){
sse[i]=sum(kmeans(d3,centers=i,nstart = 20)$withinss)
}
plot(1:10,sse,pch=20,type="b")
clust_d3<-kmeans(d3,centers=3,nstart=1)
res<- cbind(d3,clust=clust_d3$cluster)
ggplot(data = res, aes(x1,x2,color=as.factor(clust)))+ geom_point()
m<-5
dknn<-kNNdistplot(d1[,1:2],k=m)
eps<-0.5
dbclust1<-dbscan(d1[,1:2],eps=eps,minPts=m)
d1$db_clust<-dbclust1$cluster
ggplot(data = d1,aes(x1,x2,color=as.factor(db_clust)))+geom_point()
